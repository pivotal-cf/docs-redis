---
title: Configuring Automated Service Backups
owner: London Services
---

<strong><%= modified_date %></strong>

This topic describes how to configure automated backups in Redis for Pivotal Cloud Foundry (PCF).

## <a id="comparison"></a>Comparison of the Available Backup Methods

Redis for PCF provides two backup methods, which can be used together or alone:

* BOSH Backup and Restore (BBR) - <em>preferred</em>
* Automated service backups

If you have already set up BBR for your Pivotal Application Service (PAS) deployment,
you might find it easier to use BBR to back up your on-demand Redis service instances, in addition to
or instead of, using automated service backups.

The table below summarizes the differences between the two methods:

<table class="nice">
    <col width="15%">
    <col width="20%">
    <col width="65%">
    <th>Backup Method</th>
    <th>Supported Services</th>
    <th>What is Backed Up</th>
    <tr>
        <td>BBR</td>
        <td>On-demand</td>
        <td><ul><li>Data stored in Redis</li>
            <li>Manifest used to deploy service instance</li>
            <li>Certain additional configuration including:
         plan settings such as <strong>Redis Client Timeout</strong> and arbitrary parameters such as <code>maxmemory-policy</code></li></ul></td>
     </tr>
     <tr>
        <td>Automated service backups</td>
        <td><ul><li>On-demand</li><li>Shared-VM</li><li>Dedicated-VM</li></ul></td>
        <td>Data stored in Redis</td>
    </tr>
</table>

<p class="note"><strong>Note</strong>: Neither backup method backs up other manual changes made to service instances,
either via SSH or with the redis client <code>config</code> command.</p>

For more information, see [BOSH Backup and Restore (BBR) for On-Demand Redis for PCF](./bbr-backup.html).

## <a id="backup"></a>About Automated Service Backups

You can configure automatic backups for all service plan types: on-demand, dedicated-VM, and shared-VM.

Automated backups have the following features:

* Backups run on a configurable schedule.
* Every instance is backed up.
* The Redis broker state file is backed up.
* Data from Redis is flushed to disk before the backup is started by running a `BGSAVE` on each instance.
* You can configure Amazon Web Services (AWS) S3, SCP, Azure, or Google Cloud Storage (GCS) as your destination.

## <a id='backup-files'></a>Backup Files

When Redis for PCF runs an automated backup, it labels the backups in the following ways:

* For dedicated-VM and shared-VM plans, backups are labeled with timestamp, instance GUID, and plan name.
  Files are stored by date.
* For on-demand plans, backups are labeled with timestamp and plan name.
  Files are stored by deployment, then date.

For each backup artifact, Redis for PCF creates a file that contains the MD5 checksum for that artifact.
This can be used to validate that the artifact is not corrupted.

## <a id="about-configuration"></a>About Configuring Backups

Redis for PCF automatically backs up databases to external storage.

* **How and where**: There are four options for how automated backups transfer backup data and where the data saves to:

    * [Option 1: Back Up with AWS](#aws): Redis for PCF runs an AWS S3 client that saves backups to an S3 bucket.
    * [Option 2: Back Up with SCP](#scp): Redis for PCF runs an SCP command
      that secure-copies backups to a VM or physical machine operating outside of PCF.
    SCP stands for secure copy protocol, and offers a way to securely transfer files between two hosts.
    The operator provisions the backup machine separately from their PCF installation.
    This is the fastest option.
    * [Option 3: Back Up to GCS](#gcs): Redis for PCF runs an GCS SDK that saves backups to an Google Cloud Storage bucket.
    * [Option 4: Back Up to Azure](#azure): Redis for PCF runs an Azure SDK that saves backups to an Azure storage account.

* **When**: Backups follow a schedule that you specify with a cron expression.

    For general information about cron, see [package cron](http://godoc.org/github.com/robfig/cron).

To configure automated backups, follow the procedures below according to the option you choose for external storage.

## <a id="aws"></a>Option 1: Back Up with AWS

To back up your database to an Amazon S3 bucket, complete the following procedures:

* [Create a Policy and Access Key](#aws-create-policy)
* [Configure Backups in Ops Manager](#aws-config-om)

### <a id='aws-create-policy'></a> Create a Policy and Access Key

Redis for PCF accesses your S3 store through a user account.
Pivotal recommends that this account be solely for Redis for PCF.
You must apply a minimal policy that lets the user account upload backups to your S3 store.

Do the following to create a policy and access key:

1. Navigate to the AWS Console and log in.
1. To create a new custom policy, go to **IAM > Policies > Create Policy > Create Your Own Policy**
   and paste in the following permissions:

    ```
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "s3:ListBucket",
                    "s3:ListBucketMultipartUploads",
                    "s3:ListMultipartUploadParts",
                    "s3:PutObject"
                ],
                "Resource": [
                    "arn:aws:s3:::MY-BUCKET-NAME",
                    "arn:aws:s3:::MY-BUCKET-NAME/*"
                ]
            }
        ]
    }
    ```
    Where `MY-BUCKET-NAME` is the name of your S3 bucket.
    <br><br>
    If the S3 bucket does not already exist, add `s3:CreateBucket` to the `Action` list to create it.

1. (Recommended) Create a new user for Redis for PCF and record its Access Key ID and Secret Access Key, the user credentials.

1. (Recommended) Attach the policy you created to the AWS user account that Redis for PCF will use to access S3.
   Go to **IAM > Policies > Policy Actions > Attach**.

### <a id='aws-config-om'></a> Configure Backups in Ops Manager

Do the following to connect Redis for PCF to your S3 account:

1. Navigate to the Ops Manager Installation Dashboard and click the **Redis for PCF** tile.
1. Click **Backups**.
1. Under **Backup configuration**, select **AWS S3**.
    ![OpsManager Backups S3 view](s3backupoptions.png)
1. Fill in the fields as follows:

    <table class="nice">
    <col width="20%">
    <col width="60%">
    <col width="20%">
    <thead>
    <tr>
    <th>Field</th>
    <th>Description</th>
    <th>Mandatory/Optional</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>Access Key ID</td>
    <td>The access key for your S3 account</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Secret Access Key</td>
    <td>The Secret Key associated with your Access Key</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Endpoint URL</td>
    <td>The endpoint of your S3 account, such as <code>http://s3.amazonaws.com</code></td>
    <td>Optional, defaults to <code>http://s3.amazonaws.com</code> if not specified</td>
    </tr>
    <tr>
    <td>Bucket Name</td>
    <td>Name of the bucket where to store the backup</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Bucket Path</td>
    <td>Path inside the bucket to save backups to</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Cron Schedule</td>
    <td>Backups schedule in crontab format. For example, once daily at 2am is <code>* 2 * * *</code>. This field also accepts a pre-defined schedule, such as <code>@yearly</code>, <code>@monthly</code>, <code>@weekly</code>, <code>@daily</code>, <code>@hourly</code>, or <code>@every TIME</code>, where <code>TIME</code> is any supported time string, such as <code>1h30m</code>. For more information, see the cron package <a href="https://godoc.org/github.com/robfig/cron#hdr-Predefined_schedules">documentation</a>.</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Backup timeout</td>
    <td>The amount of time, in seconds, that the backup process waits for the <code>BGSAVE</code> command to complete on your instance before transferring the RDB file to your configured destination. If the timeout is reached, <code>BGSAVE</code> continues but backups fail and are not uploaded.</td>
    <td>Mandatory</td>
    </tr>
    </tbody>
    </table>

1. Click **Save**.

## <a id="scp"></a>Option 2: Back Up with SCP

To back up your database using SCP, complete the following procedures:

* [(Recommended) Create a Public and Private Key Pair](#scp-keys)
* [Configure Backups in Ops Manager](#scp-config-om)

### <a id="scp-keys"></a>(Recommended) Create a Public and Private Key Pair

Redis for PCF accesses a remote host as a user with a private key for authentication.
Pivotal recommends that this user and key pair be solely for Redis for PCF.

Do the following to create a new public and private key pair for authenticating:

1. Determine the remote host that you will be using to store backups for Redis for PCF.
   Ensure that the Redis service instances can access the remote host.
    <p class="note"><strong>Note</strong>: Pivotal recommends using a VM outside the PCF deployment for the destination of SCP backups.
    As a result you might need to enable public IPs for the Redis VMs.</p>
1. Create a new user for Redis for PCF on the destination VM.
1. Create a new public and private key pair for authenticating as the above user on the destination VM.

### <a id="scp-config-om"></a>Configure Backups in Ops Manager

Do the following to connect Redis for PCF to your destination VM:

1. Navigate to the Ops Manager Installation Dashboard and click the **Redis for PCF** tile.
1. Click **Backups**.
1. Under **Backup configuration**, select **SCP**.
    ![OpsManager Backups SCP view](scpbackupoptions.png)
1. Fill in the fields as follows:

    <table class="nice">
    <col width="20%">
    <col width="60%">
    <col width="20%">
    <thead>
    <tr>
    <th>Field</th>
    <th>Description</th>
    <th>Mandatory/Optional</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>Username</td>
    <td>The username to use for transferring backups to the SCP server</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Private Key</td>
    <td>The private SSH key of the user configured in <code>Username</code></td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Hostname</td>
    <td>The hostname or IP address of the SCP server</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Destination Directory</td>
    <td>The path in the SCP server, where the backups will be transferred</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>SCP Port</td>
    <td>The SCP port of the SCP server</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Cron Schedule</td>
    <td>Backups schedule in crontab format. For example, once daily at 2am is <code>* 2 * * *</code>. This field also accepts a pre-defined schedule, such as <code>@yearly</code>, <code>@monthly</code>, <code>@weekly</code>, <code>@daily</code>, <code>@hourly</code>, or <code>@every TIME</code>, where <code>TIME</code> is any supported time string, such as <code>1h30m</code>. For more information, see the cron package <a href="https://godoc.org/github.com/robfig/cron#hdr-Predefined_schedules">documentation</a>.</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Backup timeout</td>
    <td>The amount of time, in seconds, that the backup process waits for the <code>BGSAVE</code> command to complete on your instance before transferring the RDB file to the SCP server. If the timeout is reached, <code>BGSAVE</code> continues but backups fail and are not uploaded.</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Fingerprint</td>
    <td>The fingerprint of the public key of the SCP server. To retrieve the server's fingerprint, run <code>ssh-keygen -E md5 -lf ~/.ssh/id_rsa.pub</code>.</td>
    <td>Optional</td>
    </tr>
    </tbody>
    </table>

1. Click **Save**.

## <a id="gcs"></a>Option 3: Back Up with GCS

To back up your database using GCS, complete the following procedures:

* [Create a Service Account](#gcs-policy-access-key)
* [Configure Backups in Ops Manager](#gcs-config-om)

### <a id="gcs-policy-access-key"></a>Create a Service Account

Redis for PCF accesses your GCS store through a service account.
Pivotal recommends that this account be solely for Redis for PCF.
You must apply a minimal policy that lets the user account upload backups to your GCS store.

Do the following to create a service account with the correct permissions:

1. In the GCS console, create a new service account for Redis for PCF: **IAM and Admin > Service Accounts > Create Service Account**.
1. Enter a unique name in the **Service account name** field, such as `Redis-for-PCF`.
1. In the **Roles** dropdown, grant the new service account the **Storage Admin** role.
1. Select the **Furnish a new private key** checkbox so that a new key is created and downloaded.
1. Click **Create** and take note of the name and location of the service account JSON file that is downloaded.

### <a id="gcs-config-om"></a>Configure Backups in Ops Manager

Do the following to connect Redis for PCF to GCS:

1. Navigate to the Ops Manager Installation Dashboard and click the **Redis for PCF** tile.
1. Click **Backups**.
1. Under **Backup configuration**, select **GCS**.
    ![OpsManager Backups GCS view](gcsbackupoptions.png)
1. Fill in the fields as follows:

    <table class="nice">
    <col width="20%">
    <col width="60%">
    <col width="20%">
    <thead>
    <tr>
    <th>Field</th>
    <th>Description</th>
    <th>Mandatory/Optional</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>Project ID</td>
    <td>Google Cloud Platform (GCP) Project ID</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Bucket name</td>
    <td>Name of the bucket where to store the backup</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Service account private key</td>
    <td>The JSON secret key associated with your service account</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Cron Schedule</td>
    <td>Backups schedule in crontab format. For example, once daily at 2am is <code>* 2 * * *</code>. This field also accepts a pre-defined schedule, such as <code>@yearly</code>, <code>@monthly</code>, <code>@weekly</code>, <code>@daily</code>, <code>@hourly</code>, or <code>@every TIME</code>, where <code>TIME</code> is any supported time string, such as <code>1h30m</code>. For more information, see the cron package <a href="https://godoc.org/github.com/robfig/cron#hdr-Predefined_schedules">documentation</a>.</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Backup timeout</td>
    <td>The amount of time, in seconds, that the backup process waits for the <code>BGSAVE</code> command to complete on your instance before transferring the RDB file to your configured destination. If the timeout is reached, <code>BGSAVE</code> continues but backups fail and are not uploaded.</td>
    <td>Mandatory</td>
    </tr>
    </tbody>
    </table>

1. Click **Save**.

## <a id ="azure"></a>Back Up to Azure

Do the following to back up your database to an Azure storage account:

1. Navigate to the Ops Manager Installation Dashboard and click the **Redis for PCF** tile.
1. Click **Backups**.
1. Under **Backup configuration**, select **Azure**.
    ![OpsManager Backups Azure view](azurebackupoptions.png)
1. Fill in the fields as follows:

    <table class="nice">
    <col width="20%">
    <col width="60%">
    <col width="20%">
    <thead>
    <tr>
    <th>Field</th>
    <th>Description</th>
    <th>Mandatory/Optional</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>Account</td>
    <td>Account name</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Azure Storage Access Key</td>
    <td>Azure specific credentials required to write to the Azure container</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Container Name</td>
    <td>Name of the Azure container where to store the backup</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Destination Directory</td>
    <td>Directory within the Azure container to store the backup files to</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Blob Store Base URL</td>
    <td>URL pointing to Azure resource</td>
    <td>Optional</td>
    </tr>
    <tr>
    <td>Cron Schedule</td>
    <td>Backups schedule in crontab format. For example, once daily at 2am is <code>* 2 * * *</code>. This field also accepts a pre-defined schedule, such as <code>@yearly</code>, <code>@monthly</code>, <code>@weekly</code>, <code>@daily</code>, <code>@hourly</code>, or <code>@every TIME</code>, where <code>TIME</code> is any supported time string, such as <code>1h30m</code>. For more information, see the cron package <a href="https://godoc.org/github.com/robfig/cron#hdr-Predefined_schedules">documentation</a>.</td>
    <td>Mandatory</td>
    </tr>
    <tr>
    <td>Backup timeout</td>
    <td>The amount of time, in seconds, that the backup process waits for the <code>BGSAVE</code> command to complete on your instance before transferring the RDB file to your configured destination. If the timeout is reached, <code>BGSAVE</code> continues but backups fail and are not uploaded.</td>
    <td>Mandatory</td>
    </tr>
    </tbody>
    </table>

1. Click **Save**.

## <a id="manual-br"></a>Back Up and Restore Manually

To back up or restore Redis manually, see [Manually Backing Up and Restoring Redis for Pivotal Cloud Foundry]
(https://community.pivotal.io/s/article/Manually-Backing-Up-and-Restoring-Redis-for-Pivotal-Cloud-Foundry)
in the Pivotal Support knowledge base.
